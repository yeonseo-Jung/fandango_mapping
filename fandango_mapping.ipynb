{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245444a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import math\n",
    "from openpyxl import Workbook, load_workbook\n",
    "from openpyxl.styles import PatternFill,Color\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "# path\n",
    "cur_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "root = os.path.abspath(os.path.join(cur_dir, os.pardir))\n",
    "download = os.path.abspath(os.path.join(os.path.abspath(os.path.join(root, os.pardir)), 'Downloads'))\n",
    "fandango = os.path.join(cur_dir, 'fandango_bak')\n",
    "\n",
    "print(download, cur_dir, fandango)\n",
    "\n",
    "if not os.path.exists(fandango):\n",
    "    os.mkdir(fandango)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "now_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "last_date = datetime.today() - timedelta(days=7)\n",
    "\n",
    "# 예외적으로 일주일 이내에 작업함\n",
    "# last_date = datetime.today() - timedelta(days=6)\n",
    "\n",
    "last_date = last_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "now_date, last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_kind = input(\"Enter the kind: (1: now, 2: open)\")\n",
    "\n",
    "while True:\n",
    "    if _kind == '1':\n",
    "        kind = 'now'\n",
    "        now = f\"now_playing_update_{now_date}.xlsx\"\n",
    "        last = f\"now_playing_update_{last_date}.xlsx\"\n",
    "        save = f\"(save)now_playing_update_{now_date}.xlsx\"\n",
    "        break\n",
    "    elif _kind == '2':\n",
    "        kind = 'open'\n",
    "        now = f\"opening_this_week_{now_date}.xlsx\"\n",
    "        last = f\"opening_this_week_{last_date}.xlsx\"\n",
    "        save = f\"(save)opening_this_week_{now_date}.xlsx\"\n",
    "        break\n",
    "    else:\n",
    "        _kind = input(\"Enter the kind: (1: now, 2: open)\")\n",
    "        \n",
    "now, last, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a288723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path\n",
    "_now_path = os.path.join(download, now)\n",
    "_last_path = os.path.join(download, last)\n",
    "now_path = os.path.join(fandango, now)\n",
    "last_path = os.path.join(fandango, last)\n",
    "add_path = last_path\n",
    "\n",
    "save_path = os.path.join(fandango, save)\n",
    "\n",
    "# move file\n",
    "if os.path.exists(_now_path):\n",
    "    shutil.move(_now_path, now_path)\n",
    "if os.path.exists(_last_path):\n",
    "    shutil.move(_last_path, last_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca5a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_excel(now_path)\n",
    "res = res.astype({'release_date': 'str'})\n",
    "res.loc[:, 'release_date'] = res.release_date.str[:10]\n",
    "\n",
    "last = pd.read_excel(last_path , sheet_name='Sheet2')\n",
    "last = last.astype({'release_date': 'str'})\n",
    "last.loc[:, 'release_date'] = last.release_date.str[:10]\n",
    "\n",
    "\n",
    "res = res.drop(['Unnamed: 0'], axis=1)\n",
    "last = last.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "#0 : 중복 아님, 1: 저번주것과 중복인데 값 없음, 2: 저번주와 중복인데 값존재  ==> 중복 (1): 노란색 , (0) : 빨간색\n",
    "res['(yellow)overlap']=0 \n",
    "\n",
    "#수기 검수 때 편리하게 하기 위한 컬럼, 나중에 보고드릴 땐 삭제하고 드리면 됨\n",
    "res['fandango'] = ''\n",
    "res['tmdb'] = ''\n",
    "res['imdb'] = ''\n",
    "\n",
    "\n",
    "res['series_id'] = res['series_id'].astype(str) #id 값 문자열로 변환\n",
    "res['tmdb_id'] = res['tmdb_id'].astype(str)\n",
    "\n",
    "last['series_id'] = last['series_id'].astype(str)\n",
    "last['tmdb_id'] = last['tmdb_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b008ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저번주에 찾은 정보 대입 - sheet2 채운 값들 불러오기\n",
    "\n",
    "for a in range(0,len(res)):\n",
    "    res.loc[a, 'series_id']=res.loc[a, 'series_id'].replace('.0','')\n",
    "    res.loc[a, 'tmdb_id']=res.loc[a, 'tmdb_id'].replace('.0','')\n",
    "    for b in range(0,len(last)):\n",
    "        if res.loc[a, 'title_fandango']==last.loc[b, 'title_fandango']:\n",
    "            if res.loc[a, 'series_id'] =='nan':\n",
    "                res.loc[a, '(yellow)overlap']=1\n",
    "            else:\n",
    "                res.loc[a, '(yellow)overlap']=2  \n",
    "            break\n",
    "\n",
    "add1_info = pd.read_excel(add_path , sheet_name='Sheet2')\n",
    "add1_info = add1_info.astype({'release_date': 'str'})\n",
    "add1_info.loc[:, 'release_date'] = add1_info.release_date.str[:10]\n",
    "add1 = add1_info.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "add2_info = pd.read_excel(last_path , sheet_name='Sheet2')\n",
    "add2_info = add2_info.astype({'release_date': 'str'})\n",
    "add2_info.loc[:, 'release_date'] = add2_info.release_date.str[:10]\n",
    "add2 = add2_info.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "for a in range(0,len(add1)):\n",
    "    add1.loc[a, 'series_id']=str(add1.loc[a, 'series_id']).replace('.0','')\n",
    "    add1.loc[a, 'tmdb_id']=str(add1.loc[a, 'tmdb_id']).replace('.0','')\n",
    "    \n",
    "for a in range(0,len(add2)):\n",
    "    add2.loc[a, 'series_id']=str(add2.loc[a, 'series_id']).replace('.0','')\n",
    "    add2.loc[a, 'tmdb_id']=str(add2.loc[a, 'tmdb_id']).replace('.0','')\n",
    "\n",
    "for a in range(0,len(res)):\n",
    "    for b in range(0,len(add1)):\n",
    "        if res.loc[a, 'title_fandango'] == add1.loc[b, 'title_fandango']:\n",
    "            res.loc[a, 'imdb_id'] = add1.loc[b, 'imdb_id']\n",
    "            res.loc[a, 'series_id'] = add1.loc[b, 'series_id']\n",
    "            res.loc[a, 'tmdb_id'] = add1.loc[b, 'tmdb_id']\n",
    "            res.loc[a, 'title'] = add1.loc[b, 'title']\n",
    "            res.loc[a, 'is_now_playing'] = add1.loc[b, 'is_now_playing']\n",
    "            res.loc[a, 'release_date'] = add1.loc[b, 'release_date']\n",
    "            break\n",
    "\n",
    "for a in range(0,len(res)):\n",
    "    for b in range(0,len(add2)):\n",
    "        if res.loc[a, 'title_fandango'] == add2.loc[b, 'title_fandango']:\n",
    "            res.loc[a, 'imdb_id'] = add1.loc[b, 'imdb_id']\n",
    "            res.loc[a, 'series_id'] = add2.loc[b, 'series_id']\n",
    "            res.loc[a, 'tmdb_id'] = add2.loc[b, 'tmdb_id']\n",
    "            res.loc[a, 'title'] = add2.loc[b, 'title']\n",
    "            res.loc[a, 'is_now_playing'] = add2.loc[b, 'is_now_playing']\n",
    "            res.loc[a, 'release_date'] = add2.loc[b, 'release_date']\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4951c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmdb 검색시 y:2022 를 추가하면 2022년도에 가까운 순으로 검색 결과가 나옴\n",
    "# 해당 기능을 이용하기 위한 코드\n",
    "\n",
    "def tmdb_url_find(url):\n",
    "    t=' ' \n",
    "    year=' '\n",
    "    for a in range(0,len(url)):\n",
    "        if url[a] =='(' and url[a+5]==')':\n",
    "            year = url[a+1:a+5]\n",
    "            year = 'y:'+year\n",
    "            break\n",
    "        else:\n",
    "            t = t+url[a]\n",
    "            \n",
    "    return t+year\n",
    "\n",
    "# nan 값을 가지고 있는 셀에 사이트 링크 연결 ==> 검색 하는 시간 단축을 위함 , 검색이 안될 수도 있음\n",
    "for a in range(len(res)):\n",
    "    url = res.loc[a, 'title_fandango']\n",
    "    \n",
    "    # fandango url\n",
    "    res.loc[a, 'fandango'] = 'https://www.fandango.com'+res.loc[a, 'ticketing_url']\n",
    "    \n",
    "    # tmdb url\n",
    "    tmdb_url = tmdb_url_find(url).replace(' ','%20')\n",
    "    res.loc[a, 'tmdb'] = 'https://www.themoviedb.org/search/movie?query='+tmdb_url\n",
    "    \n",
    "    # imdb url\n",
    "    imdb_id = res.loc[a, 'imdb_id']\n",
    "    if imdb_id == \"none\":\n",
    "        res.loc[a, 'imdb'] = \"https://www.imdb.com/find?q=\"+url.replace(' ','+')+'+&ref_=nv_sr_sm'\n",
    "    else:\n",
    "        res.loc[a, 'imdb'] = \"https://www.imdb.com/title/\" + imdb_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_temp = os.path.join(fandango, 'fandango_mapping_temporary_file_temp.xlsx')\n",
    "res.to_excel(_temp)\n",
    "wb = load_workbook(filename=_temp)\n",
    "ws = wb.active\n",
    "\n",
    "for a in range(0,len(res)):\n",
    "    \n",
    "    if res.loc[a, '(yellow)overlap'] ==0:  \n",
    "        for col_num in range(1,11):\n",
    "            color = Color(indexed = 2)\n",
    "            paint_cell = PatternFill(patternType='solid', fgColor=color)\n",
    "            \n",
    "            select_cell = ws.cell(row=a+2, column=col_num)\n",
    "            select_cell.fill = paint_cell        \n",
    "    elif res.loc[a, '(yellow)overlap'] ==1:  \n",
    "        for col_num in range(1,11):\n",
    "            color = Color(indexed = 5)\n",
    "            paint_cell = PatternFill(patternType='solid', fgColor=color)\n",
    "            \n",
    "            select_cell = ws.cell(row=a+2, column=col_num)\n",
    "            select_cell.fill = paint_cell\n",
    "            \n",
    "ws.delete_cols(11)\n",
    "wb.save(save_path)\n",
    "\n",
    "os.remove(_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603919dd",
   "metadata": {},
   "source": [
    "---\n",
    "### Mapping: tmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "# import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scrapping\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from user_agent import generate_user_agent\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.webdriver.common.alert import Alert\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium.webdriver.common.action_chains import ActionChains\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.common.exceptions import UnexpectedAlertPresentException, NoAlertPresentException, NoSuchElementException, TimeoutException\n",
    "\n",
    "# Exception Error Handling\n",
    "import socket\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_url(url, window=False, image=False):\n",
    "    ''' Set up webdriver, useragent & Get url '''\n",
    "    \n",
    "    wd = None\n",
    "    socket.setdefaulttimeout(30)\n",
    "    error = []\n",
    "    attempts = 0 # url parsing 시도횟수\n",
    "    # 10번 이상 parsing 실패시 pass\n",
    "    while attempts < 10:\n",
    "        try:  \n",
    "            attempts += 1\n",
    "            # user agent\n",
    "            options = Options() \n",
    "            userAgent = generate_user_agent(os=('mac', 'linux'), navigator='chrome', device_type='desktop')\n",
    "            options.add_argument('window-size=1920x1080')\n",
    "            options.add_argument(\"--disable-gpu\")\n",
    "            options.add_argument('--disable-extensions')\n",
    "            options.add_argument(f'user-agent={userAgent}')\n",
    "            \n",
    "            if not window:\n",
    "                options.add_argument('headless')\n",
    "            if not image:\n",
    "                options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "\n",
    "            # web driver \n",
    "            wd = webdriver.Chrome(ChromeDriverManager().install(), chrome_options=options)\n",
    "            wd.get(url)\n",
    "            wd.implicitly_wait(5)\n",
    "            break\n",
    "\n",
    "        # 예외처리\n",
    "        except Exception as e:\n",
    "            print(f'\\n\\nError: {str(e)}\\n\\n')\n",
    "            \n",
    "            time.sleep(30)\n",
    "            try:\n",
    "                wd.quit()\n",
    "            except:\n",
    "                pass\n",
    "            wd = None\n",
    "    return wd\n",
    "    \n",
    "def scroll_down(wd, sleep_time, check_count):\n",
    "    ''' page scroll down '''\n",
    "    \n",
    "    cnt = 0\n",
    "    while True:\n",
    "        height = wd.execute_script(\"return document.body.scrollHeight\")\n",
    "        wd.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "        time.sleep(sleep_time)\n",
    "        cnt += 1\n",
    "        if cnt == check_count:\n",
    "            break        \n",
    "    return wd\n",
    "\n",
    "def get_headers():\n",
    "    userAgent = generate_user_agent(os=('mac', 'linux'), navigator='chrome', device_type='desktop')\n",
    "    headers = {\n",
    "        \"user-agent\": userAgent,\n",
    "        \"Accept\": \"application/json\",\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "def json_iterator(url, iterations=5, headers=True):\n",
    "    \n",
    "    if headers:\n",
    "        headers = get_headers()\n",
    "    else:\n",
    "        headers = None\n",
    "    cnt = 0\n",
    "    res_data = None\n",
    "    while cnt <= iterations:\n",
    "        try:\n",
    "            res = requests.get(url, headers=headers)\n",
    "            res_data = json.loads(res.text)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')\n",
    "        cnt += 1\n",
    "        time.sleep(100)\n",
    "        \n",
    "    return res_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf17f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hangle._distance import jamo_levenshtein\n",
    "\n",
    "month = {\n",
    "    'January': '01',\n",
    "    'February': '02',\n",
    "    'March': '03',\n",
    "    'April': '04',\n",
    "    'May': '05',\n",
    "    'June': '06',\n",
    "    'July': '07',\n",
    "    'August': '08',\n",
    "    'September': '09',\n",
    "    'October': '10',\n",
    "    'November': '11',\n",
    "    'December': '12',\n",
    "}\n",
    "\n",
    "movie_df = res.drop(columns='(yellow)overlap').copy()\n",
    "movie_df.loc[movie_df.series_id==\"nan\", [\"series_id\", \"tmdb_id\"]] = None, None\n",
    "for idx in tqdm(range(len(movie_df))):\n",
    "    title_fandango_org = movie_df.loc[idx, 'title_fandango']\n",
    "    express = re.compile(r'[\\(]+[0-9]+[\\)]+')\n",
    "    title_fandango = re.sub(express, '', title_fandango_org).strip()\n",
    "    _tmdb_id = movie_df.loc[idx, 'tmdb_id']\n",
    "    url = movie_df.loc[idx, 'tmdb']\n",
    "\n",
    "    if _tmdb_id is not None:\n",
    "        movie_df.loc[idx, ['similarity', 'status_mapping']] = 1, 1\n",
    "        continue\n",
    "    else:\n",
    "        wd = get_url(url, window=False, image=False)\n",
    "        time.sleep(1.5)\n",
    "        html = BeautifulSoup(wd.page_source, 'lxml')\n",
    "        movies = html.find_all('div', 'title')\n",
    "        status_mapping = 0\n",
    "        tmdbs, sims = [], []\n",
    "        mapping = False\n",
    "        for movie in movies:\n",
    "            \n",
    "            # tmdb id\n",
    "            tmdb_id = movie.find('a')['href']\n",
    "            if '/movie/' in tmdb_id:\n",
    "                tmdb_id = tmdb_id.replace('/movie/', '').strip()\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # release_date\n",
    "            if movie.find('span') is None:\n",
    "                release_date = ''    \n",
    "            else:\n",
    "                release_date = movie.find('span').text.split()\n",
    "                if len(release_date) == 3:\n",
    "                    m = month[release_date[0]]\n",
    "                    d = release_date[1].replace(',', '')\n",
    "                    if len(d) == 1:\n",
    "                        d = '0' + d\n",
    "                    y = release_date[2]\n",
    "                    release_date = f'{y}-{m}-{d}'\n",
    "                else:\n",
    "                    release_date = '' \n",
    "                    \n",
    "            # title\n",
    "            if movie.find('a') is None:\n",
    "                title = ''\n",
    "            else:\n",
    "                title_org = movie.find('a').text.strip()\n",
    "                express = re.compile(r'[\\(]+[0-9]+[\\)]+')\n",
    "                title = re.sub(express, '', title_org).strip()\n",
    "\n",
    "            # Check similarity\n",
    "            status_mapping = 1\n",
    "            a = title.replace(' ', '').lower()\n",
    "            b = title_fandango.replace(' ', '').lower()\n",
    "            dist = jamo_levenshtein(a, b)\n",
    "            max_len = max(len(a), len(b))\n",
    "            sim = (max_len - dist) / max_len\n",
    "            \n",
    "            if sim == 1:\n",
    "                mapping = True\n",
    "                break\n",
    "            sims.append(sim)\n",
    "            print(title_fandango, tmdb_id, title, release_date, sim)\n",
    "            tmdbs.append([tmdb_id, title, release_date, sim])   \n",
    "        \n",
    "        if status_mapping == 0:\n",
    "            movie_df.loc[idx, ['series_id', 'tmdb_id', 'title', 'release_date', 'similarity', 'status_mapping']] = np.nan, np.nan, np.nan, '', np.nan, status_mapping\n",
    "        else:\n",
    "            if mapping:\n",
    "                max_sim = sim\n",
    "            else:\n",
    "                max_sim = max(sims)\n",
    "                max_idx = sims.index(max_sim)\n",
    "                max_tmdbs = tmdbs[max_idx]\n",
    "                tmdb_id, tmdb_id, title, release_date = max_tmdbs[0], max_tmdbs[0], max_tmdbs[1], max_tmdbs[2]\n",
    "            \n",
    "            if max_sim == 1:\n",
    "                # 맵핑 완료\n",
    "                status_mapping = 1\n",
    "            elif max_sim >= 0.9:\n",
    "                # 맵핑 후보 (수기검수 필요)\n",
    "                status_mapping = 2\n",
    "            else:\n",
    "                # 맵핑 안됨 (수기검수 필요)\n",
    "                status_mapping = 3\n",
    "                \n",
    "            movie_df.loc[idx, ['series_id', 'tmdb_id', 'title', 'release_date', 'similarity', 'status_mapping']] = tmdb_id, tmdb_id, title, release_date, max_sim, status_mapping\n",
    "        \n",
    "        movie_df.loc[movie_df.release_date=='nan', 'release_date'] = \"\"\n",
    "        wd.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968948b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.groupby('status_mapping').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddfae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.groupby('similarity').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e61c1",
   "metadata": {},
   "source": [
    "---\n",
    "### Mapping: imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d745f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in tqdm(range(len(movie_df))):\n",
    "\n",
    "    title_fandango_org = movie_df.loc[idx, 'title_fandango']\n",
    "    express = re.compile(r'[\\(]+[0-9]+[\\)]+')\n",
    "    title_fandango = re.sub(express, '', title_fandango_org).strip()\n",
    "    url = movie_df.loc[idx, 'imdb']\n",
    "    _imdb_id = movie_df.loc[idx, 'imdb_id']\n",
    "    if _imdb_id != 'none':\n",
    "        movie_df.loc[idx, ['similarity_imdb', 'status_mapping_imdb']] = 1, 1\n",
    "    else:\n",
    "        wd = get_url(url, window=False, image=False)\n",
    "        time.sleep(2.5)\n",
    "        \n",
    "        html = BeautifulSoup(wd.page_source, 'lxml')\n",
    "        movies = html.find_all('td', 'result_text')\n",
    "        if len(movies) == 0:\n",
    "            movies = html.find_all('div', 'ipc-metadata-list-summary-item__tc')\n",
    "        \n",
    "        status_mapping = 0\n",
    "        imdbs, sims = [], []\n",
    "        mapping = False\n",
    "        for movie in movies:\n",
    "            # imdb_id\n",
    "            imdb_id = movie.find('a')['href'].split('/')[2]\n",
    "\n",
    "            # title\n",
    "            title_imdb = movie.find('a').text.strip()\n",
    "            express = re.compile(r'[\\(]+[0-9]+[\\)]+')\n",
    "            title_imdb = re.sub(express, '', title_imdb).strip()\n",
    "            \n",
    "            # Check similarity\n",
    "            status_mapping = 1\n",
    "            a = title_imdb.replace(' ', '').lower()\n",
    "            b = title_fandango.replace(' ', '').lower()\n",
    "            dist = jamo_levenshtein(a, b)\n",
    "            max_len = max(len(a), len(b))\n",
    "            sim = (max_len - dist) / max_len\n",
    "            \n",
    "            print(title_fandango, imdb_id, title_imdb, sim)\n",
    "            if sim == 1:\n",
    "                mapping = True\n",
    "                break\n",
    "            sims.append(sim)\n",
    "            imdbs.append([imdb_id, title_imdb, sim])\n",
    "\n",
    "\n",
    "        if status_mapping == 0:\n",
    "            movie_df.loc[idx, ['imdb_id', 'title_imdb', 'similarity_imdb', 'status_mapping_imdb']] = None, \"\", np.nan, status_mapping\n",
    "        else:\n",
    "            if mapping:\n",
    "                max_sim = sim\n",
    "            else:\n",
    "                max_sim = max(sims)\n",
    "                max_idx = sims.index(max_sim)\n",
    "                max_imdbs = imdbs[max_idx]\n",
    "                imdb_id, title_imdb = max_imdbs[0], max_imdbs[1]\n",
    "                \n",
    "            if max_sim == 1:\n",
    "                # 맵핑 완료\n",
    "                status_mapping = 1\n",
    "            elif max_sim >= 0.9:\n",
    "                # 맵핑 후보 (수기검수 필요)\n",
    "                status_mapping = 2\n",
    "            else:\n",
    "                # 맵핑 안됨 (수기검수 필요)\n",
    "                status_mapping = 3\n",
    "                \n",
    "            movie_df.loc[idx, ['imdb_id', 'title_imdb', 'similarity_imdb', 'status_mapping_imdb']] = imdb_id, title_imdb, max_sim, status_mapping\n",
    "            \n",
    "        wd.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.groupby(\"status_mapping_imdb\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.groupby(\"similarity_imdb\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19b5066",
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = ['fandango', 'tmdb', 'imdb', 'similarity', 'status_mapping']\n",
    "\n",
    "# movie_df_1 = movie_df[(movie_df.status_mapping==1) | (movie_df.status_mapping==2)].drop(columns=drops).reset_index(drop=True)\n",
    "movie_df_1 = movie_df[(movie_df.status_mapping==1) | (movie_df.status_mapping==2)].reset_index(drop=True)\n",
    "movie_df_1.loc[:, 'is_now_playing'] = 1\n",
    "\n",
    "# movie_df_0 = movie_df[(movie_df.status_mapping==0) | (movie_df.status_mapping==3)].drop(columns=drops).reset_index(drop=True)\n",
    "movie_df_0 = movie_df[(movie_df.status_mapping==0) | (movie_df.status_mapping==3)].reset_index(drop=True)\n",
    "movie_df_0.loc[:, 'is_now_playing'] = ''\n",
    "\n",
    "# origin file\n",
    "_movie_df = pd.read_excel(now_path).iloc[:, 1:]\n",
    "\n",
    "\n",
    "# Save file\n",
    "now_path_final = os.path.join(cur_dir, now)\n",
    "writer = pd.ExcelWriter(now_path_final, engine = 'openpyxl')\n",
    "_movie_df.to_excel(writer, sheet_name = 'Sheet1')\n",
    "movie_df_1.to_excel(writer, sheet_name = 'Sheet2')\n",
    "movie_df_0.to_excel(writer, sheet_name = 'Sheet3')\n",
    "\n",
    "writer.save()\n",
    "writer.close()\n",
    "\n",
    "print(f\"Complete save file:  {now_path_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check page\n",
    "\n",
    "# for idx in movie_df[movie_df.status_mapping!=1].index:\n",
    "#     title_fandango = movie_df.loc[idx, 'title_fandango']\n",
    "#     print(title_fandango)\n",
    "    \n",
    "#     url_tm = movie_df.loc[idx, 'tmdb']\n",
    "#     url_im = movie_df.loc[idx, 'imdb']\n",
    "#     wd_tm = get_url(url_tm, True, True)\n",
    "#     wd_im = get_url(url_im, True, True)\n",
    "    \n",
    "#     c = input('Close?(Enter the any key): ')\n",
    "    \n",
    "#     wd_tm.quit()\n",
    "#     wd_im.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "549c0af12cda6a489d7ec7271bb42e11470c0bdcea6fb0094f863c6635a382f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
